{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T12:41:40.965441Z",
     "start_time": "2019-05-04T12:41:40.485519Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnf = sc.getConf()\\\n",
    ".set('spark.cores.max', '4')\\\n",
    ".set('spark.max.memory', '2g')\\\n",
    ".set('spark.executor.memory', '2g')\n",
    "sc.stop()\n",
    "sc = SparkContext(conf=cnf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T07:35:38.570262Z",
     "start_time": "2019-05-05T07:35:38.564377Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "%matplotlib inline\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "data_dir = '../../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### some structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-01T03:22:27.303417Z",
     "start_time": "2019-05-01T03:22:27.300223Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df = spark.read.csv(data_dir, header=True, inferSchema=True)\n",
    "# df = pd.read_csv(data_path, engine='python')\n",
    "# df['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-01T03:22:27.313707Z",
     "start_time": "2019-05-01T03:22:27.305649Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class DataVO(object):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.stock_code = kwargs['stock_code']\n",
    "        self.date_time = kwargs['date_time']\n",
    "        self.change = kwargs['change']\n",
    "        self.price = kwargs['price']\n",
    "        self.volume = kwargs['volume']\n",
    "        self.amount = kwargs['amount']\n",
    "        self.typ = kwargs['typ']\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(f\"stock record for stock: {self.stock_code}, at time: {self.date_time}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_schema() -> StructType:\n",
    "        \"\"\"\n",
    "        Get DataFrame schema\n",
    "        :return: StructType\n",
    "        \"\"\"\n",
    "        fields = ['time', 'stock_code', 'price', 'change', 'volume', 'amount', 'type']\n",
    "        field_types = [TimestampType(), DoubleType(), DoubleType(), DoubleType(), DoubleType(), DoubleType(),\n",
    "                       DoubleType()]\n",
    "        sfields = []\n",
    "        for f, t in zip(fields, field_types):\n",
    "            sfields.append(StructField(f, t, True))\n",
    "        schema = StructType(sfields)\n",
    "        return schema\n",
    "\n",
    "def ecapsulate(line: list):\n",
    "    date_time = line[0]\n",
    "    price = line[1]\n",
    "    change = line[2]\n",
    "    volume = line[3]\n",
    "    amount = line[4]\n",
    "    typ = line[5]\n",
    "    stock_code = line[6]\n",
    "    return DataVO(stock_code=stock_code, \n",
    "                 date_time=date_time, \n",
    "                 price=price, \n",
    "                 change=change, \n",
    "                 volume=volume, \n",
    "                 amount=amount, \n",
    "                 typ=typ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T14:11:49.515586Z",
     "start_time": "2019-04-12T14:11:49.512657Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T14:11:49.782381Z",
     "start_time": "2019-04-12T14:11:49.771379Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10stock_5min_data.csv',\n",
       " '10stock_tick_data.csv',\n",
       " '10stock_tick_data.csv.zip',\n",
       " 'test_data.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '../../data'\n",
    "data_path = pjoin(data_dir, '10stock_tick_data.csv')\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T14:11:50.955137Z",
     "start_time": "2019-04-12T14:11:50.379862Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "../../data/10stock_tick_data.csv MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile(data_path)\n",
    "rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T03:36:50.862108Z",
     "start_time": "2019-04-12T03:36:47.912286Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datetime,price,change,volume,amount,type,stock_code',\n",
       " '2019-03-29 09:25:04,10.82,-0.09,816,882912,in,000001',\n",
       " '2019-03-29 09:30:00,10.92,0.1,136,148252,in,000001',\n",
       " '2019-03-29 09:30:03,10.84,-0.08,542,588171,mid,000001',\n",
       " '2019-03-29 09:30:06,10.84,0.0,45,48782,out,000001',\n",
       " '2019-03-29 09:30:10,10.88,0.04,40,43421,in,000001',\n",
       " '2019-03-29 09:30:12,10.87,-0.01,14,15218,mid,000001',\n",
       " '2019-03-29 09:30:16,10.88,0.01,154,167455,in,000001',\n",
       " '2019-03-29 09:30:21,10.9,0.02,20,21790,in,000001',\n",
       " '2019-03-29 09:30:24,10.88,-0.02,38,41414,out,000001']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T03:46:44.076041Z",
     "start_time": "2019-04-12T03:46:44.073133Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# rdd = sc.parallelize(['2019-03-29 09:25:04,10.82,-0.09,816,882912,in,000001\\n2019-03-29 09:30:00,10.92,0.1,136,148252,in,000001'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T03:46:44.281704Z",
     "start_time": "2019-04-12T03:46:44.278930Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# rdd.flatMap(lambda lines: lines.split('\\n')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T03:46:44.767500Z",
     "start_time": "2019-04-12T03:46:44.760178Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lines = rdd.flatMap(lambda lines: lines.split('\\n'))\n",
    "field_lines = lines.map(lambda line: line.split(','))\n",
    "# cast string to column schema\n",
    "def cast_types(line):\n",
    "    try:\n",
    "        l = [dt.strptime(line[0], '%Y-%m-%d %X'), float(line[1]), float(line[2]), float(line[3]), float(line[4]), line[5], line[6]]\n",
    "    except ValueError as e:\n",
    "        l = [dt(2000, 1, 1, 0, 0, 0) , -1.0,  -1.0, -1.0, -1.0, '-1', '-1']\n",
    "    return l\n",
    "field_lines_casted = field_lines.map(cast_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T03:46:45.308660Z",
     "start_time": "2019-04-12T03:46:45.305849Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# field_lines_casted.collect()\n",
    "# str(ecapsulate(range(7)))\n",
    "# lines.map(lambda line: line.split(',')).map(lambda rec: str(ecapsulate(rec))).collect()\n",
    "# lines.map(lambda line: line.split(',')).map(lambda rec: ecapsulate(rec)).map(lambda obj: obj.price).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T03:46:45.580270Z",
     "start_time": "2019-04-12T03:46:45.575742Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create schema\n",
    "fields = ['time','price','change','volume','amount','type','stock_code']\n",
    "field_types = [TimestampType(), DoubleType(), DoubleType(), DoubleType(), DoubleType(), StringType(), StringType()]\n",
    "sfields = []\n",
    "for f, t in zip(fields, field_types):\n",
    "    sfields.append(StructField(f, t, True))\n",
    "schema = StructType(sfields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T03:46:48.908066Z",
     "start_time": "2019-04-12T03:46:46.239863Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df = spark.createDataFrame(data=field_lines, schema=schema)\n",
    "df = field_lines_casted.toDF(schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T03:46:50.499994Z",
     "start_time": "2019-04-12T03:46:48.910334Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+------+------+--------+----+----------+\n",
      "|               time|price|change|volume|  amount|type|stock_code|\n",
      "+-------------------+-----+------+------+--------+----+----------+\n",
      "|2000-01-01 00:00:00| -1.0|  -1.0|  -1.0|    -1.0|  -1|        -1|\n",
      "|2019-03-29 09:25:04|10.82| -0.09| 816.0|882912.0|  in|    000001|\n",
      "|2019-03-29 09:30:00|10.92|   0.1| 136.0|148252.0|  in|    000001|\n",
      "|2019-03-29 09:30:03|10.84| -0.08| 542.0|588171.0| mid|    000001|\n",
      "|2019-03-29 09:30:06|10.84|   0.0|  45.0| 48782.0| out|    000001|\n",
      "|2019-03-29 09:30:10|10.88|  0.04|  40.0| 43421.0|  in|    000001|\n",
      "|2019-03-29 09:30:12|10.87| -0.01|  14.0| 15218.0| mid|    000001|\n",
      "|2019-03-29 09:30:16|10.88|  0.01| 154.0|167455.0|  in|    000001|\n",
      "|2019-03-29 09:30:21| 10.9|  0.02|  20.0| 21790.0|  in|    000001|\n",
      "|2019-03-29 09:30:24|10.88| -0.02|  38.0| 41414.0| out|    000001|\n",
      "+-------------------+-----+------+------+--------+----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T03:47:00.469871Z",
     "start_time": "2019-04-12T03:46:52.915562Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|stock_code|        mean_price|\n",
      "+----------+------------------+\n",
      "|    000008|3.8767889908256885|\n",
      "|    000006|  5.88334991708123|\n",
      "|    000007| 8.551960326721115|\n",
      "|    000011|11.003074204947003|\n",
      "|    000010| 3.658470588235287|\n",
      "|    000009| 4.743333333333331|\n",
      "|    000001|11.003074204947003|\n",
      "|    000002|26.504330808080837|\n",
      "|    000005|3.0191596638655436|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"stock\")\n",
    "# spark.sql('select t.stock_code, mean(t.price) as mean_price from stock t where t.volume > 150 group by t.stock_code').show()\n",
    "spark.sql('select t.stock_code, mean(t.price) as mean_price from stock t where t.volume > 150 group by t.stock_code').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T08:59:05.192926Z",
     "start_time": "2019-04-12T08:59:02.871795Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|stock_code|        mean_price|\n",
      "+----------+------------------+\n",
      "|    000008|3.8767889908256885|\n",
      "|    000006|  5.88334991708123|\n",
      "|    000007| 8.551960326721115|\n",
      "|    000011|11.003074204947003|\n",
      "|    000010| 3.658470588235287|\n",
      "|    000009| 4.743333333333331|\n",
      "|    000001|11.003074204947003|\n",
      "|    000002|26.504330808080837|\n",
      "|    000005|3.0191596638655436|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('volume > 150').groupby('stock_code').mean('price').withColumnRenamed('avg(price)', 'mean_price').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test = sc.parallelize(['2019-03-29 09:25:04'])\n",
    "test.map(lambda x: dt.strptime('2019-03-29 09:25:04', '%Y-%m-%d %X')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T02:52:26.604173Z",
     "start_time": "2019-04-30T02:52:26.600918Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime as dt\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T02:52:27.151218Z",
     "start_time": "2019-04-30T02:52:27.146361Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kmeans_data.txt',\n",
       " '10stock_5min_data.csv',\n",
       " '10stock_tick_data.csv',\n",
       " '10stock_tick_data.csv.zip',\n",
       " 'streaming_kmeans_data_test.txt',\n",
       " '997stock_3day_tick_data.csv',\n",
       " 'kmeans_data_test.txt',\n",
       " 'test_data.csv']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '../../data'\n",
    "data_path = pjoin(data_dir, '10stock_tick_data.csv')\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T02:21:43.117056Z",
     "start_time": "2019-04-30T02:21:43.092620Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(data_path)\n",
    "# file_length = rdd.map(lambda x: 1).reduce(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T02:21:43.387277Z",
     "start_time": "2019-04-30T02:21:43.325769Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# split the rdd into 5 equal-size parts\n",
    "rddQueue = rdd.randomSplit(range(10), seed=5003)\n",
    "# Create a StreamingContext with batch interval of 5 seconds\n",
    "ssc = StreamingContext(sc, 5)\n",
    "# Feed the rdd queue to a DStream\n",
    "rdds = ssc.queueStream(rddQueue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T02:21:43.849717Z",
     "start_time": "2019-04-30T02:21:43.843264Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create schema\n",
    "fields = ['time','price','change','volume','amount','type','stock_code']\n",
    "field_types = [TimestampType(), DoubleType(), DoubleType(), DoubleType(), DoubleType(), StringType(), StringType()]\n",
    "sfields = []\n",
    "for f, t in zip(fields, field_types):\n",
    "    sfields.append(StructField(f, t, True))\n",
    "schema = StructType(sfields)\n",
    "\n",
    "# loading data\n",
    "lines = rdds.flatMap(lambda lines: lines.split('\\n'))\n",
    "field_lines = lines.map(lambda line: line.split(','))\n",
    "field_lines_casted = field_lines.map(lambda line: [dt.strptime(line[0], '%Y-%m-%d %X'), float(line[1]), float(line[2]), float(line[3]), float(line[4]), line[5], line[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T02:21:44.337933Z",
     "start_time": "2019-04-30T02:21:44.332230Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert RDDs of the words DStream to DataFrame and run SQL query\n",
    "def process(time, rdd):\n",
    "    print(\"========= %s =========\" % str(time))\n",
    "\n",
    "    try:\n",
    "        # Get the singleton instance of SparkSession\n",
    "#         spark = getSparkSessionInstance(rdd.context.getConf())\n",
    "        df = rdd.toDF(schema=schema)\n",
    "        print(f\"for time: {time}\")\n",
    "        print(f\"{'-'*30}{time}{'-'*30}\")\n",
    "        df.createOrReplaceTempView(\"stock\")\n",
    "        ss.sql('select mean(t.price) as mean_price from stock t where t.volume > 150').show()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:09:01.349621Z",
     "start_time": "2019-04-11T10:09:01.321697Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "field_lines_casted.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T10:09:36.179594Z",
     "start_time": "2019-04-11T10:09:08.942829Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2019-04-11 18:09:09 =========\n",
      "for time: 2019-04-11 18:09:09\n",
      "------------------------------2019-04-11 18:09:09------------------------------\n",
      "========= 2019-04-11 18:09:10 =========\n",
      "for time: 2019-04-11 18:09:10\n",
      "------------------------------2019-04-11 18:09:10------------------------------\n",
      "========= 2019-04-11 18:09:11 =========\n",
      "for time: 2019-04-11 18:09:11\n",
      "------------------------------2019-04-11 18:09:11------------------------------\n",
      "========= 2019-04-11 18:09:12 =========\n",
      "for time: 2019-04-11 18:09:12\n",
      "------------------------------2019-04-11 18:09:12------------------------------\n",
      "========= 2019-04-11 18:09:13 =========\n",
      "for time: 2019-04-11 18:09:13\n",
      "------------------------------2019-04-11 18:09:13------------------------------\n",
      "========= 2019-04-11 18:09:14 =========\n",
      "for time: 2019-04-11 18:09:14\n",
      "------------------------------2019-04-11 18:09:14------------------------------\n",
      "========= 2019-04-11 18:09:15 =========\n",
      "for time: 2019-04-11 18:09:15\n",
      "------------------------------2019-04-11 18:09:15------------------------------\n",
      "========= 2019-04-11 18:09:16 =========\n",
      "for time: 2019-04-11 18:09:16\n",
      "------------------------------2019-04-11 18:09:16------------------------------\n",
      "========= 2019-04-11 18:09:17 =========\n",
      "for time: 2019-04-11 18:09:17\n",
      "------------------------------2019-04-11 18:09:17------------------------------\n",
      "========= 2019-04-11 18:09:18 =========\n",
      "for time: 2019-04-11 18:09:18\n",
      "------------------------------2019-04-11 18:09:18------------------------------\n",
      "========= 2019-04-11 18:09:19 =========\n",
      "for time: 2019-04-11 18:09:19\n",
      "------------------------------2019-04-11 18:09:19------------------------------\n",
      "========= 2019-04-11 18:09:20 =========\n",
      "for time: 2019-04-11 18:09:20\n",
      "------------------------------2019-04-11 18:09:20------------------------------\n",
      "========= 2019-04-11 18:09:21 =========\n",
      "for time: 2019-04-11 18:09:21\n",
      "------------------------------2019-04-11 18:09:21------------------------------\n",
      "========= 2019-04-11 18:09:22 =========\n",
      "for time: 2019-04-11 18:09:22\n",
      "------------------------------2019-04-11 18:09:22------------------------------\n",
      "========= 2019-04-11 18:09:23 =========\n",
      "for time: 2019-04-11 18:09:23\n",
      "------------------------------2019-04-11 18:09:23------------------------------\n",
      "========= 2019-04-11 18:09:24 =========\n",
      "for time: 2019-04-11 18:09:24\n",
      "------------------------------2019-04-11 18:09:24------------------------------\n",
      "========= 2019-04-11 18:09:25 =========\n",
      "for time: 2019-04-11 18:09:25\n",
      "------------------------------2019-04-11 18:09:25------------------------------\n",
      "========= 2019-04-11 18:09:26 =========\n",
      "for time: 2019-04-11 18:09:26\n",
      "------------------------------2019-04-11 18:09:26------------------------------\n",
      "========= 2019-04-11 18:09:27 =========\n",
      "for time: 2019-04-11 18:09:27\n",
      "------------------------------2019-04-11 18:09:27------------------------------\n",
      "========= 2019-04-11 18:09:28 =========\n",
      "for time: 2019-04-11 18:09:28\n",
      "------------------------------2019-04-11 18:09:28------------------------------\n",
      "========= 2019-04-11 18:09:29 =========\n",
      "for time: 2019-04-11 18:09:29\n",
      "------------------------------2019-04-11 18:09:29------------------------------\n",
      "========= 2019-04-11 18:09:30 =========\n",
      "for time: 2019-04-11 18:09:30\n",
      "------------------------------2019-04-11 18:09:30------------------------------\n",
      "========= 2019-04-11 18:09:31 =========\n",
      "for time: 2019-04-11 18:09:31\n",
      "------------------------------2019-04-11 18:09:31------------------------------\n",
      "========= 2019-04-11 18:09:32 =========\n",
      "for time: 2019-04-11 18:09:32\n",
      "------------------------------2019-04-11 18:09:32------------------------------\n",
      "========= 2019-04-11 18:09:33 =========\n",
      "for time: 2019-04-11 18:09:33\n",
      "------------------------------2019-04-11 18:09:33------------------------------\n",
      "========= 2019-04-11 18:09:34 =========\n",
      "for time: 2019-04-11 18:09:34\n",
      "------------------------------2019-04-11 18:09:34------------------------------\n",
      "========= 2019-04-11 18:09:35 =========\n",
      "for time: 2019-04-11 18:09:35\n",
      "------------------------------2019-04-11 18:09:35------------------------------\n",
      "========= 2019-04-11 18:09:36 =========\n",
      "for time: 2019-04-11 18:09:36\n",
      "------------------------------2019-04-11 18:09:36------------------------------\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o116.awaitTermination.\n: java.lang.NullPointerException\r\n\tat org.apache.spark.storage.BlockManagerMaster.removeRdd(BlockManagerMaster.scala:126)\r\n\tat org.apache.spark.SparkContext.unpersistRDD(SparkContext.scala:1819)\r\n\tat org.apache.spark.rdd.RDD.unpersist(RDD.scala:217)\r\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$clearMetadata$3.apply(DStream.scala:458)\r\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$clearMetadata$3.apply(DStream.scala:457)\r\n\tat scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:139)\r\n\tat scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:139)\r\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\r\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\r\n\tat scala.collection.mutable.HashMap$$anon$2.foreach(HashMap.scala:139)\r\n\tat org.apache.spark.streaming.dstream.DStream.clearMetadata(DStream.scala:457)\r\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$clearMetadata$5.apply(DStream.scala:470)\r\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$clearMetadata$5.apply(DStream.scala:470)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.streaming.dstream.DStream.clearMetadata(DStream.scala:470)\r\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$clearMetadata$2.apply(DStreamGraph.scala:134)\r\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$clearMetadata$2.apply(DStreamGraph.scala:134)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.streaming.DStreamGraph.clearMetadata(DStreamGraph.scala:134)\r\n\tat org.apache.spark.streaming.scheduler.JobGenerator.clearMetadata(JobGenerator.scala:263)\r\n\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:184)\r\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)\r\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-18f3db416f1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Development\\spark\\python\\pyspark\\streaming\\context.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    190\u001b[0m         \"\"\"\n\u001b[0;32m    191\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Development\\spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Development\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Development\\spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     transform(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o116.awaitTermination.\n: java.lang.NullPointerException\r\n\tat org.apache.spark.storage.BlockManagerMaster.removeRdd(BlockManagerMaster.scala:126)\r\n\tat org.apache.spark.SparkContext.unpersistRDD(SparkContext.scala:1819)\r\n\tat org.apache.spark.rdd.RDD.unpersist(RDD.scala:217)\r\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$clearMetadata$3.apply(DStream.scala:458)\r\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$clearMetadata$3.apply(DStream.scala:457)\r\n\tat scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:139)\r\n\tat scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:139)\r\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\r\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\r\n\tat scala.collection.mutable.HashMap$$anon$2.foreach(HashMap.scala:139)\r\n\tat org.apache.spark.streaming.dstream.DStream.clearMetadata(DStream.scala:457)\r\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$clearMetadata$5.apply(DStream.scala:470)\r\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$clearMetadata$5.apply(DStream.scala:470)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.streaming.dstream.DStream.clearMetadata(DStream.scala:470)\r\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$clearMetadata$2.apply(DStreamGraph.scala:134)\r\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$clearMetadata$2.apply(DStreamGraph.scala:134)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.streaming.DStreamGraph.clearMetadata(DStreamGraph.scala:134)\r\n\tat org.apache.spark.streaming.scheduler.JobGenerator.clearMetadata(JobGenerator.scala:263)\r\n\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:184)\r\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)\r\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2019-04-11 18:09:37 =========\n",
      "for time: 2019-04-11 18:09:37\n",
      "------------------------------2019-04-11 18:09:37------------------------------\n",
      "========= 2019-04-11 18:09:38 =========\n",
      "for time: 2019-04-11 18:09:38\n",
      "------------------------------2019-04-11 18:09:38------------------------------\n",
      "========= 2019-04-11 18:09:39 =========\n",
      "for time: 2019-04-11 18:09:39\n",
      "------------------------------2019-04-11 18:09:39------------------------------\n",
      "========= 2019-04-11 18:09:40 =========\n",
      "for time: 2019-04-11 18:09:40\n",
      "------------------------------2019-04-11 18:09:40------------------------------\n",
      "========= 2019-04-11 18:09:41 =========\n",
      "for time: 2019-04-11 18:09:41\n",
      "------------------------------2019-04-11 18:09:41------------------------------\n",
      "========= 2019-04-11 18:09:42 =========\n",
      "for time: 2019-04-11 18:09:42\n",
      "------------------------------2019-04-11 18:09:42------------------------------\n",
      "========= 2019-04-11 18:09:43 =========\n",
      "for time: 2019-04-11 18:09:43\n",
      "------------------------------2019-04-11 18:09:43------------------------------\n",
      "========= 2019-04-11 18:09:44 =========\n",
      "for time: 2019-04-11 18:09:44\n",
      "------------------------------2019-04-11 18:09:44------------------------------\n",
      "========= 2019-04-11 18:09:45 =========\n",
      "for time: 2019-04-11 18:09:45\n",
      "------------------------------2019-04-11 18:09:45------------------------------\n"
     ]
    }
   ],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination(30)\n",
    "ssc.stop(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-11T08:26:48.403Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clustering --> aggregation --> regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T12:41:41.679433Z",
     "start_time": "2019-05-04T12:41:41.272805Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.clustering import StreamingKMeans\n",
    "from pyspark.mllib.regression import StreamingLinearRegressionWithSGD\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id, lag\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from datetime import timedelta\n",
    "from datetime import datetime as dt\n",
    "from os.path import join as pjoin\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime as dt\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T03:29:30.774966Z",
     "start_time": "2019-04-30T03:29:30.772230Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "##### create / preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "sort the data by `time asc, stock_code asc` using codes below:\n",
    "```\n",
    "##### use pandas to do the time column refactor\n",
    "\n",
    "def refactorTime(t):\n",
    "#    formant = '%Y-%m-%d %H:%M:%S'\n",
    "#    tf = dt.strptime(t, formant)\n",
    "    tf = t\n",
    "    sec = tf.second\n",
    "    res = sec % 5\n",
    "    if res >= 3:\n",
    "        td = timedelta(seconds=(5 - res))\n",
    "        tf += td\n",
    "    else:\n",
    "        td = timedelta(seconds=(res))\n",
    "        tf -= td\n",
    "    return tf #tf.strftime(formant)\n",
    "\n",
    "data_pd = pd.read_csv(pjoin(data_dir, '997stock_3day_tick_daata.csv'))\n",
    "data_pd = data_pd.sort_values(['time', 'stock_code'])\n",
    "data_pd.reset_index(drop=True, inplace=True)\n",
    "data_pd['time'] = pd.to_datetime(data_pd['time'])\n",
    "data_pd['time'] = data_pd['time'].apply(refactorTime)\n",
    "data_pd.to_csv(pjoin(data_dir, '997stock_3day_tick_data_sorted_refactor_time.csv'), index=False)\n",
    "# data_pd.groupby(by=['time', 'stock_code'], as_index=False).agg(np.average)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "group by `'time', 'stock_code'` using code belos\n",
    "\n",
    "```\n",
    "##### use spark to do the group by\n",
    "\n",
    "fields = ['time','price','change','volume','amount','type','stock_code']\n",
    "field_types = [TimestampType(), DoubleType(), DoubleType(), DoubleType(), DoubleType(), DoubleType(), DoubleType()]\n",
    "sfields = []\n",
    "for f, t in zip(fields, field_types):\n",
    "    sfields.append(StructField(f, t, True))\n",
    "schema = StructType(sfields)\n",
    "\n",
    "data_df = spark.read.csv(pjoin(data_dir, '997stock_3day_tick_data_sorted_refactor_time.csv'), schema=schema, header=True)\n",
    "\n",
    "data_df = data_df.groupBy(col('time'), col('stock_code'))\\\n",
    ".avg('price', 'change', 'volume', 'amount', 'type')\\\n",
    ".select(col('time'), col('stock_code'),\n",
    "        bround('avg(price)', 2).alias('price'), \n",
    "        bround('avg(change)', 2).alias('change'),\n",
    "        bround('avg(volume)', 2).alias('volume'),\n",
    "        bround('avg(amount)', 2).alias('amount'),\n",
    "        bround('avg(type)', 2).alias('type'))\\\n",
    ".orderBy(col('time'), col('stock_code'), ascending=[1, 1])\n",
    "data_df.cache()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "saving data by Pandas\n",
    "\n",
    "```\n",
    "##### collect data & save\n",
    "\n",
    "data_pd = data_df.toPandas()\n",
    "data_pd = data_pd.query(\"time>'2018-12-10 09:30:00'\").reset_index(drop=True)\n",
    "data_pd.to_csv(pjoin(data_dir, '997stock_3day_tick_data_sortby_time.csv'), index=False)\n",
    "# data_pd.to_csv(pjoin(data_dir, '997stock_3day_tick_data_sortby_time.csv'), index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T07:36:06.857991Z",
     "start_time": "2019-05-05T07:36:00.157716Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_pd = pd.read_csv(pjoin(data_dir, '997stock_3day_tick_data_sortby_time.csv'))\n",
    "data_pd = data_pd.sort_values(['time', 'stock_code'])\n",
    "data_pd.reset_index(drop=True, inplace=True)\n",
    "data_pd['time'] = pd.to_datetime(data_pd['time'])\n",
    "# data_pd['time'] = data_pd['time'].apply(refactorTime)\n",
    "# data_pd.to_csv(pjoin(data_dir, '997stock_3day_tick_data_sorted_refactor_time.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T07:49:48.992619Z",
     "start_time": "2019-05-05T07:49:48.831652Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "time_idx = pd.date_range(start='2018-12-10 09:25:05', end='2018-12-10 15:00:00', freq='5s')\n",
    "time_idx = time_idx.append(pd.date_range(start='2018-12-11 09:25:05', end='2018-12-11 15:00:00', freq='5s'))\n",
    "time_idx_df = pd.DataFrame(time_idx, columns=['time'])\n",
    "time_idx_df.set_index('time', inplace=True)\n",
    "data_pd_t = data_pd.set_index('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T08:19:42.739850Z",
     "start_time": "2019-05-05T08:19:42.653639Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_pd_small = data_pd.query('stock_code < 100').reset_index(drop=True)\n",
    "# data_pd_small.to_csv(pjoin(data_dir, '997stock_3day_tick_data_sortby_time_small.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T08:38:38.043579Z",
     "start_time": "2019-05-05T08:38:09.947262Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_pd = data_pd.query(\"time>'2018-12-10 09:30:00'\").reset_index(drop=True)\n",
    "data_pd.to_csv(pjoin(data_dir, '997stock_3day_tick_data_sortby_time.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T08:39:31.056261Z",
     "start_time": "2019-05-05T08:39:30.983810Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>stock_code</th>\n",
       "      <th>price</th>\n",
       "      <th>change</th>\n",
       "      <th>volume</th>\n",
       "      <th>amount</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-10 09:30:05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3229.0</td>\n",
       "      <td>3297594.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>2018-12-10 09:30:10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>111.0</td>\n",
       "      <td>113295.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710</th>\n",
       "      <td>2018-12-10 09:30:15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>54.0</td>\n",
       "      <td>55099.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2380</th>\n",
       "      <td>2018-12-10 09:30:20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>625.0</td>\n",
       "      <td>638000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3135</th>\n",
       "      <td>2018-12-10 09:30:25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>81.0</td>\n",
       "      <td>82644.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3846</th>\n",
       "      <td>2018-12-10 09:30:30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.21</td>\n",
       "      <td>0.01</td>\n",
       "      <td>109.0</td>\n",
       "      <td>111287.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4442</th>\n",
       "      <td>2018-12-10 09:30:35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>239.0</td>\n",
       "      <td>243984.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5207</th>\n",
       "      <td>2018-12-10 09:30:40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>376.0</td>\n",
       "      <td>383506.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5948</th>\n",
       "      <td>2018-12-10 09:30:45</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>58.0</td>\n",
       "      <td>59187.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6579</th>\n",
       "      <td>2018-12-10 09:30:50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>106.5</td>\n",
       "      <td>108593.5</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7323</th>\n",
       "      <td>2018-12-10 09:30:55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1030.5</td>\n",
       "      <td>1051187.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8071</th>\n",
       "      <td>2018-12-10 09:31:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>145.0</td>\n",
       "      <td>147934.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8624</th>\n",
       "      <td>2018-12-10 09:31:05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>72.5</td>\n",
       "      <td>74001.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9430</th>\n",
       "      <td>2018-12-10 09:31:10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.22</td>\n",
       "      <td>0.01</td>\n",
       "      <td>343.0</td>\n",
       "      <td>350374.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10811</th>\n",
       "      <td>2018-12-10 09:31:20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>73.5</td>\n",
       "      <td>75104.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11537</th>\n",
       "      <td>2018-12-10 09:31:25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>90.5</td>\n",
       "      <td>92462.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12171</th>\n",
       "      <td>2018-12-10 09:31:30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.22</td>\n",
       "      <td>0.01</td>\n",
       "      <td>48.0</td>\n",
       "      <td>49055.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12824</th>\n",
       "      <td>2018-12-10 09:31:35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.5</td>\n",
       "      <td>10731.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13598</th>\n",
       "      <td>2018-12-10 09:31:40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>460.0</td>\n",
       "      <td>469974.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14342</th>\n",
       "      <td>2018-12-10 09:31:45</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.21</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14307.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14944</th>\n",
       "      <td>2018-12-10 09:31:50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>509.0</td>\n",
       "      <td>520198.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15686</th>\n",
       "      <td>2018-12-10 09:31:55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>49.0</td>\n",
       "      <td>50090.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16369</th>\n",
       "      <td>2018-12-10 09:32:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>161.0</td>\n",
       "      <td>164486.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16930</th>\n",
       "      <td>2018-12-10 09:32:05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>86.5</td>\n",
       "      <td>88408.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17723</th>\n",
       "      <td>2018-12-10 09:32:10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>72.5</td>\n",
       "      <td>74095.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19126</th>\n",
       "      <td>2018-12-10 09:32:20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.0</td>\n",
       "      <td>105316.5</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19878</th>\n",
       "      <td>2018-12-10 09:32:25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>54.0</td>\n",
       "      <td>55205.5</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20624</th>\n",
       "      <td>2018-12-10 09:32:30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.24</td>\n",
       "      <td>0.02</td>\n",
       "      <td>84.0</td>\n",
       "      <td>85940.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21352</th>\n",
       "      <td>2018-12-10 09:32:35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>43.5</td>\n",
       "      <td>44544.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22177</th>\n",
       "      <td>2018-12-10 09:32:40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>174.0</td>\n",
       "      <td>178131.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3716375</th>\n",
       "      <td>2018-12-12 14:54:35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.23</td>\n",
       "      <td>0.01</td>\n",
       "      <td>111.0</td>\n",
       "      <td>113542.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3717062</th>\n",
       "      <td>2018-12-12 14:54:40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.22</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18413.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3718157</th>\n",
       "      <td>2018-12-12 14:54:50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1601.0</td>\n",
       "      <td>1636255.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3718767</th>\n",
       "      <td>2018-12-12 14:54:55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>65.5</td>\n",
       "      <td>66942.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3719397</th>\n",
       "      <td>2018-12-12 14:55:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9198.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3719876</th>\n",
       "      <td>2018-12-12 14:55:05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>641.0</td>\n",
       "      <td>655532.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3720549</th>\n",
       "      <td>2018-12-12 14:55:10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12264.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3721149</th>\n",
       "      <td>2018-12-12 14:55:15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>330.0</td>\n",
       "      <td>337290.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3721658</th>\n",
       "      <td>2018-12-12 14:55:20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.5</td>\n",
       "      <td>41404.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3722309</th>\n",
       "      <td>2018-12-12 14:55:25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>570.0</td>\n",
       "      <td>582981.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3722914</th>\n",
       "      <td>2018-12-12 14:55:30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.22</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29863.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3723375</th>\n",
       "      <td>2018-12-12 14:55:35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>34.5</td>\n",
       "      <td>35261.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3724025</th>\n",
       "      <td>2018-12-12 14:55:40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.22</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40910.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3724618</th>\n",
       "      <td>2018-12-12 14:55:45</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>36.0</td>\n",
       "      <td>36827.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3725141</th>\n",
       "      <td>2018-12-12 14:55:50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>58.5</td>\n",
       "      <td>59845.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3725781</th>\n",
       "      <td>2018-12-12 14:55:55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>571.5</td>\n",
       "      <td>584094.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3726397</th>\n",
       "      <td>2018-12-12 14:56:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>144.0</td>\n",
       "      <td>147312.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3726868</th>\n",
       "      <td>2018-12-12 14:56:05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>209.0</td>\n",
       "      <td>213740.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3727544</th>\n",
       "      <td>2018-12-12 14:56:10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>43.5</td>\n",
       "      <td>44500.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3728187</th>\n",
       "      <td>2018-12-12 14:56:15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>71.0</td>\n",
       "      <td>72633.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3728705</th>\n",
       "      <td>2018-12-12 14:56:20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>912.0</td>\n",
       "      <td>933014.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3729371</th>\n",
       "      <td>2018-12-12 14:56:25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.5</td>\n",
       "      <td>11769.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3729970</th>\n",
       "      <td>2018-12-12 14:56:30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22506.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3730478</th>\n",
       "      <td>2018-12-12 14:56:35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.5</td>\n",
       "      <td>10751.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3731223</th>\n",
       "      <td>2018-12-12 14:56:40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>36.5</td>\n",
       "      <td>37339.5</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3731903</th>\n",
       "      <td>2018-12-12 14:56:45</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3069.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3732437</th>\n",
       "      <td>2018-12-12 14:56:50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>396.5</td>\n",
       "      <td>406016.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3733102</th>\n",
       "      <td>2018-12-12 14:56:55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5632.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3733709</th>\n",
       "      <td>2018-12-12 14:57:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3734177</th>\n",
       "      <td>2018-12-12 15:00:05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>2060288.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7283 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       time  stock_code  price  change  volume     amount  \\\n",
       "0       2018-12-10 09:30:05         1.0  10.21    0.00  3229.0  3297594.5   \n",
       "973     2018-12-10 09:30:10         1.0  10.21    0.00   111.0   113295.0   \n",
       "1710    2018-12-10 09:30:15         1.0  10.21    0.00    54.0    55099.5   \n",
       "2380    2018-12-10 09:30:20         1.0  10.20    0.00   625.0   638000.0   \n",
       "3135    2018-12-10 09:30:25         1.0  10.20    0.00    81.0    82644.5   \n",
       "3846    2018-12-10 09:30:30         1.0  10.21    0.01   109.0   111287.0   \n",
       "4442    2018-12-10 09:30:35         1.0  10.20    0.00   239.0   243984.5   \n",
       "5207    2018-12-10 09:30:40         1.0  10.20    0.00   376.0   383506.5   \n",
       "5948    2018-12-10 09:30:45         1.0  10.21    0.00    58.0    59187.0   \n",
       "6579    2018-12-10 09:30:50         1.0  10.20    0.00   106.5   108593.5   \n",
       "7323    2018-12-10 09:30:55         1.0  10.21    0.00  1030.5  1051187.5   \n",
       "8071    2018-12-10 09:31:00         1.0  10.21    0.00   145.0   147934.0   \n",
       "8624    2018-12-10 09:31:05         1.0  10.20    0.00    72.5    74001.0   \n",
       "9430    2018-12-10 09:31:10         1.0  10.22    0.01   343.0   350374.5   \n",
       "10811   2018-12-10 09:31:20         1.0  10.22    0.00    73.5    75104.0   \n",
       "11537   2018-12-10 09:31:25         1.0  10.22    0.00    90.5    92462.0   \n",
       "12171   2018-12-10 09:31:30         1.0  10.22    0.01    48.0    49055.0   \n",
       "12824   2018-12-10 09:31:35         1.0  10.22    0.00    10.5    10731.0   \n",
       "13598   2018-12-10 09:31:40         1.0  10.22    0.00   460.0   469974.5   \n",
       "14342   2018-12-10 09:31:45         1.0  10.21   -0.01    14.0    14307.0   \n",
       "14944   2018-12-10 09:31:50         1.0  10.22    0.00   509.0   520198.0   \n",
       "15686   2018-12-10 09:31:55         1.0  10.22    0.00    49.0    50090.0   \n",
       "16369   2018-12-10 09:32:00         1.0  10.22    0.00   161.0   164486.0   \n",
       "16930   2018-12-10 09:32:05         1.0  10.22    0.00    86.5    88408.0   \n",
       "17723   2018-12-10 09:32:10         1.0  10.22    0.00    72.5    74095.0   \n",
       "19126   2018-12-10 09:32:20         1.0  10.22    0.00   103.0   105316.5   \n",
       "19878   2018-12-10 09:32:25         1.0  10.22    0.00    54.0    55205.5   \n",
       "20624   2018-12-10 09:32:30         1.0  10.24    0.02    84.0    85940.0   \n",
       "21352   2018-12-10 09:32:35         1.0  10.24    0.00    43.5    44544.0   \n",
       "22177   2018-12-10 09:32:40         1.0  10.24    0.00   174.0   178131.0   \n",
       "...                     ...         ...    ...     ...     ...        ...   \n",
       "3716375 2018-12-12 14:54:35         1.0  10.23    0.01   111.0   113542.0   \n",
       "3717062 2018-12-12 14:54:40         1.0  10.22   -0.01    18.0    18413.0   \n",
       "3718157 2018-12-12 14:54:50         1.0  10.22    0.00  1601.0  1636255.0   \n",
       "3718767 2018-12-12 14:54:55         1.0  10.22    0.00    65.5    66942.0   \n",
       "3719397 2018-12-12 14:55:00         1.0  10.22    0.00     9.0     9198.0   \n",
       "3719876 2018-12-12 14:55:05         1.0  10.22    0.00   641.0   655532.0   \n",
       "3720549 2018-12-12 14:55:10         1.0  10.22    0.00    12.0    12264.0   \n",
       "3721149 2018-12-12 14:55:15         1.0  10.22    0.00   330.0   337290.0   \n",
       "3721658 2018-12-12 14:55:20         1.0  10.23    0.00    40.5    41404.5   \n",
       "3722309 2018-12-12 14:55:25         1.0  10.23    0.00   570.0   582981.5   \n",
       "3722914 2018-12-12 14:55:30         1.0  10.22   -0.01    29.0    29863.0   \n",
       "3723375 2018-12-12 14:55:35         1.0  10.23    0.00    34.5    35261.5   \n",
       "3724025 2018-12-12 14:55:40         1.0  10.22   -0.01    40.0    40910.0   \n",
       "3724618 2018-12-12 14:55:45         1.0  10.23    0.00    36.0    36827.0   \n",
       "3725141 2018-12-12 14:55:50         1.0  10.23    0.00    58.5    59845.5   \n",
       "3725781 2018-12-12 14:55:55         1.0  10.23    0.00   571.5   584094.0   \n",
       "3726397 2018-12-12 14:56:00         1.0  10.23    0.00   144.0   147312.0   \n",
       "3726868 2018-12-12 14:56:05         1.0  10.23    0.00   209.0   213740.0   \n",
       "3727544 2018-12-12 14:56:10         1.0  10.23    0.00    43.5    44500.0   \n",
       "3728187 2018-12-12 14:56:15         1.0  10.23    0.00    71.0    72633.0   \n",
       "3728705 2018-12-12 14:56:20         1.0  10.23    0.00   912.0   933014.0   \n",
       "3729371 2018-12-12 14:56:25         1.0  10.24    0.00    11.5    11769.5   \n",
       "3729970 2018-12-12 14:56:30         1.0  10.23    0.00    22.0    22506.0   \n",
       "3730478 2018-12-12 14:56:35         1.0  10.24    0.00    10.5    10751.0   \n",
       "3731223 2018-12-12 14:56:40         1.0  10.23    0.00    36.5    37339.5   \n",
       "3731903 2018-12-12 14:56:45         1.0  10.23    0.00     3.0     3069.0   \n",
       "3732437 2018-12-12 14:56:50         1.0  10.24    0.00   396.5   406016.0   \n",
       "3733102 2018-12-12 14:56:55         1.0  10.24    0.00     5.5     5632.0   \n",
       "3733709 2018-12-12 14:57:00         1.0  10.24    0.00     2.0     2048.0   \n",
       "3734177 2018-12-12 15:00:05         1.0  10.24    0.00  2012.0  2060288.0   \n",
       "\n",
       "         type  \n",
       "0         0.0  \n",
       "973       1.0  \n",
       "1710      1.0  \n",
       "2380      0.0  \n",
       "3135      0.0  \n",
       "3846      1.0  \n",
       "4442      0.0  \n",
       "5207      0.0  \n",
       "5948      1.0  \n",
       "6579     -0.5  \n",
       "7323      1.0  \n",
       "8071      1.0  \n",
       "8624      0.0  \n",
       "9430      1.0  \n",
       "10811     1.0  \n",
       "11537     0.0  \n",
       "12171     1.0  \n",
       "12824     1.0  \n",
       "13598     1.0  \n",
       "14342    -1.0  \n",
       "14944     1.0  \n",
       "15686    -1.0  \n",
       "16369    -1.0  \n",
       "16930    -1.0  \n",
       "17723    -1.0  \n",
       "19126    -1.0  \n",
       "19878    -1.0  \n",
       "20624     1.0  \n",
       "21352     1.0  \n",
       "22177     0.0  \n",
       "...       ...  \n",
       "3716375   1.0  \n",
       "3717062  -1.0  \n",
       "3718157  -1.0  \n",
       "3718767  -1.0  \n",
       "3719397  -1.0  \n",
       "3719876  -1.0  \n",
       "3720549  -1.0  \n",
       "3721149  -1.0  \n",
       "3721658   0.0  \n",
       "3722309   1.0  \n",
       "3722914  -1.0  \n",
       "3723375   0.0  \n",
       "3724025  -1.0  \n",
       "3724618   1.0  \n",
       "3725141   1.0  \n",
       "3725781   1.0  \n",
       "3726397   1.0  \n",
       "3726868   0.0  \n",
       "3727544   1.0  \n",
       "3728187   1.0  \n",
       "3728705  -1.0  \n",
       "3729371   0.0  \n",
       "3729970  -1.0  \n",
       "3730478   1.0  \n",
       "3731223  -1.0  \n",
       "3731903  -1.0  \n",
       "3732437   1.0  \n",
       "3733102   1.0  \n",
       "3733709   1.0  \n",
       "3734177  -1.0  \n",
       "\n",
       "[7283 rows x 7 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pd.query(\"stock_code == 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "run command below to create a smaller version of data file to do the test\n",
    "\n",
    "`head -100000 997stock_3day_tick_data_sortby_time.csv > 997stock_3day_tick_data_sortby_time_small.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load data as rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### rdd functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T12:42:06.367350Z",
     "start_time": "2019-05-04T12:42:06.358431Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def refactorTime(t):\n",
    "    formant = '%Y-%m-%d %H:%M:%S'\n",
    "    tf = dt.strptime(t, formant)\n",
    "    return tf #tf.strftime(formant)\n",
    "\n",
    "def flattenLine(v):\n",
    "    r = []\n",
    "#     r.append(v[0])\n",
    "    for l in v:\n",
    "        if isinstance(l, list) or isinstance(l, tuple):\n",
    "            r.extend(flattenLine(l))\n",
    "        else:\n",
    "            r.append(l)\n",
    "    return r\n",
    "\n",
    "def split(s):\n",
    "    kvs = s.split(',')\n",
    "    # make seconds to 0 or 30.\n",
    "    k = refactorTime(kvs[0])\n",
    "    vs = [float(item) for item in kvs[1:]]\n",
    "    return [k, vs]\n",
    "\n",
    "def rddq(rdd, keys, st, ed):\n",
    "    if (ed > len(keys)):\n",
    "        ed = len(keys) - 1\n",
    "        return rdd.filter(lambda v: v[0] >= keys[st] and v[0] <= keys[ed]).map(flattenLine)\n",
    "    else:\n",
    "        return rdd.filter(lambda v: v[0] >= keys[st] and v[0] < keys[ed]).map(flattenLine)\n",
    "    \n",
    "def rddg(rdd, keys, step):\n",
    "    for i in range(0, len(keys), step):\n",
    "        yield rddq(rdd, keys, i, i+step).map(flattenLine)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### rdd operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T12:42:13.145767Z",
     "start_time": "2019-05-04T12:42:08.640320Z"
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(pjoin(data_dir, '997stock_3day_tick_data_sortby_time_small.csv'))\n",
    "rdd.cache()\n",
    "rdd = rdd.mapPartitionsWithIndex(lambda idx, it: islice(it, 1, None) if idx == 0 else it)\n",
    "rdd = rdd.map(split)\n",
    "\n",
    "keynum = rdd.keys().distinct().count()\n",
    "keys = sorted(rdd.keys().distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T12:42:14.190126Z",
     "start_time": "2019-05-04T12:42:13.148861Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.datetime(2018, 12, 10, 9, 30, 5), 2539.0, 4.57, 0.02, 39.0, 17834.0, 0.5]\n",
      "[[datetime.datetime(2018, 12, 10, 9, 30, 10), 1.0, 10.21, 0.0, 111.0, 113295.0, 1.0]]\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "rddQueue = list(rddg(rdd, keys, 3))\n",
    "print(rddQueue[0].collect()[-1])\n",
    "print(rddQueue[1].take(1))\n",
    "# print(keys)\n",
    "print(len(rddQueue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### dataFrame operations for API test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`please ignore this part`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-01T12:12:56.013918Z",
     "start_time": "2019-05-01T12:12:56.010893Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rdd_test = rddQueue[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-01T12:12:57.157494Z",
     "start_time": "2019-05-01T12:12:56.946418Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----+------+------+---------+----+\n",
      "|               time|stock_code|price|change|volume|   amount|type|\n",
      "+-------------------+----------+-----+------+------+---------+----+\n",
      "|2018-12-10 09:25:05|       1.0|10.22| -0.06|8905.0|9101002.0|-1.0|\n",
      "|2018-12-10 09:25:05|       2.0|25.12| -0.22|2522.0|6335264.0|-1.0|\n",
      "|2018-12-10 09:25:05|       5.0|  3.0| -0.02|  47.0|  14100.0|-1.0|\n",
      "|2018-12-10 09:25:05|       6.0| 5.66| -0.05| 639.0| 361674.0|-1.0|\n",
      "|2018-12-10 09:25:05|       7.0| 7.83| -0.17|1035.0| 810405.0| 1.0|\n",
      "|2018-12-10 09:25:05|       8.0| 3.89| -0.03| 569.0| 221341.0|-1.0|\n",
      "|2018-12-10 09:25:05|       9.0| 4.78|   0.0| 256.0| 122368.0|-1.0|\n",
      "|2018-12-10 09:25:05|      10.0| 3.84| -0.05|1153.0| 442752.0|-1.0|\n",
      "|2018-12-10 09:25:05|      11.0| 10.5| -0.08|1386.0|1455300.0|-1.0|\n",
      "|2018-12-10 09:25:05|      12.0|  4.2| -0.01| 152.0|  63840.0|-1.0|\n",
      "|2018-12-10 09:25:05|      14.0|10.23| -0.24|4792.0|4902216.0|-1.0|\n",
      "|2018-12-10 09:25:05|      16.0| 3.65| -0.02| 238.0|  86870.0|-1.0|\n",
      "|2018-12-10 09:25:05|      17.0| 5.23| -0.04|4524.0|2366052.0| 1.0|\n",
      "|2018-12-10 09:25:05|      18.0| 2.46| -0.03|1438.0| 353748.0| 1.0|\n",
      "|2018-12-10 09:25:05|      19.0| 7.76| -0.04| 723.0| 561048.0|-1.0|\n",
      "|2018-12-10 09:25:05|      20.0|12.21| -0.39|1883.0|2299143.0|-1.0|\n",
      "|2018-12-10 09:25:05|      21.0| 6.08| -0.03| 129.0|  78432.0|-1.0|\n",
      "|2018-12-10 09:25:05|      23.0|13.19| -0.19|1127.0|1486513.0| 1.0|\n",
      "|2018-12-10 09:25:05|      25.0|28.29| -0.18| 159.0| 449811.0|-1.0|\n",
      "|2018-12-10 09:25:05|      26.0| 7.44| -0.08|   1.0|    744.0|-1.0|\n",
      "+-------------------+----------+-----+------+------+---------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create schema\n",
    "fields = ['time','stock_code', 'price','change','volume','amount','type']\n",
    "field_types = [TimestampType(), DoubleType(), DoubleType(), DoubleType(), DoubleType(), DoubleType(), DoubleType()]\n",
    "sfields = []\n",
    "for f, t in zip(fields, field_types):\n",
    "    sfields.append(StructField(f, t, True))\n",
    "schema = StructType(sfields)\n",
    "\n",
    "# df = spark.createDataFrame(data=rdd, schema=schema)\n",
    "df = rdd_test.toDF(schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-01T12:13:01.015377Z",
     "start_time": "2019-05-01T12:12:59.538762Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----+------+------+---------+----+\n",
      "|               time|stock_code|price|change|volume|   amount|type|\n",
      "+-------------------+----------+-----+------+------+---------+----+\n",
      "|2018-12-10 09:25:05|       1.0|10.22| -0.06|8905.0|9101002.0|-1.0|\n",
      "|2018-12-10 09:30:05|       1.0|10.21|   0.0|3229.0|3297594.5| 0.0|\n",
      "|2018-12-10 09:25:05|       2.0|25.12| -0.22|2522.0|6335264.0|-1.0|\n",
      "|2018-12-10 09:30:00|       2.0|25.12|   0.0|  49.0| 123129.0|-1.0|\n",
      "|2018-12-10 09:30:05|       2.0|25.16|  0.01| 379.5| 954694.0| 0.0|\n",
      "|2018-12-10 09:30:05|       4.0|16.17| -0.06|   7.5|  12145.5|-1.0|\n",
      "|2018-12-10 09:25:05|       5.0|  3.0| -0.02|  47.0|  14100.0|-1.0|\n",
      "|2018-12-10 09:30:05|       5.0| 2.99| -0.01| 113.0|  33698.5|-1.0|\n",
      "|2018-12-10 09:25:05|       6.0| 5.66| -0.05| 639.0| 361674.0|-1.0|\n",
      "|2018-12-10 09:30:05|       6.0| 5.64| -0.01| 982.5| 555991.5|-1.0|\n",
      "|2018-12-10 09:25:05|       7.0| 7.83| -0.17|1035.0| 810405.0| 1.0|\n",
      "|2018-12-10 09:30:00|       7.0| 7.83|   0.0|  23.0|  18009.0|-1.0|\n",
      "|2018-12-10 09:30:05|       7.0| 7.82| -0.01|1025.0| 800953.0|-0.5|\n",
      "|2018-12-10 09:25:05|       8.0| 3.89| -0.03| 569.0| 221341.0|-1.0|\n",
      "|2018-12-10 09:30:05|       8.0| 3.88|   0.0| 524.5| 203609.5| 0.0|\n",
      "|2018-12-10 09:25:05|       9.0| 4.78|   0.0| 256.0| 122368.0|-1.0|\n",
      "|2018-12-10 09:30:05|       9.0| 4.75| -0.02|  68.0|  32293.5| 0.0|\n",
      "|2018-12-10 09:25:05|      10.0| 3.84| -0.05|1153.0| 442752.0|-1.0|\n",
      "|2018-12-10 09:30:00|      10.0| 3.85|  0.01|  18.0|   6930.0| 1.0|\n",
      "|2018-12-10 09:30:05|      10.0| 3.85|   0.0| 321.0| 123428.0| 1.0|\n",
      "+-------------------+----------+-----+------+------+---------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# do aggregation\n",
    "\n",
    "# df.createOrReplaceTempView(\"curr\")\n",
    "# df = spark.sql(\"select t.time as time, \" +\n",
    "# \"t.stock_code as stock_code, \" +\n",
    "# \"round(avg(t.price),2) as price, \" +\n",
    "# \"round(avg(t.change),2) as change, \" +\n",
    "# \"round(avg(t.volume),2) as volume, \" +\n",
    "# \"round(avg(t.amount),2) as amount, \" +\n",
    "# \"round(avg(t.type),2) as type \" +\n",
    "# \"from curr t \" +\n",
    "# \"group by t.time, t.stock_code \" + \n",
    "# \"order by t.stock_code asc, t.time asc\")\n",
    "# df.show()\n",
    "\n",
    "df = df.groupBy(col('time'), col('stock_code'))\\\n",
    ".avg('price', 'change', 'volume', 'amount', 'type')\\\n",
    ".select(col('time'), col('stock_code'),\n",
    "        bround('avg(price)', 2).alias('price'), \n",
    "        bround('avg(change)', 2).alias('change'),\n",
    "        bround('avg(volume)', 2).alias('volume'),\n",
    "        bround('avg(amount)', 2).alias('amount'),\n",
    "        bround('avg(type)', 2).alias('type'))\\\n",
    ".orderBy(col('stock_code'), col('time'), ascending=[1, 1])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-01T12:13:07.187384Z",
     "start_time": "2019-05-01T12:13:03.227722Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----+------+------+---------+----+-------+----------+-----------+-----------+-----------+---------+----------+-----------+-----------+-----------+---------+\n",
      "|               time|stock_code|price|change|volume|   amount|type|lead1-Y|price-lag1|change-lag1|volume-lag1|amount-lag1|type-lag1|price-lag2|change-lag2|volume-lag2|amount-lag2|type-lag2|\n",
      "+-------------------+----------+-----+------+------+---------+----+-------+----------+-----------+-----------+-----------+---------+----------+-----------+-----------+-----------+---------+\n",
      "|2018-12-10 09:25:05|       1.0|10.22| -0.06|8905.0|9101002.0|-1.0|  10.21|      null|       null|       null|       null|     null|      null|       null|       null|       null|     null|\n",
      "|2018-12-10 09:30:05|       1.0|10.21|   0.0|3229.0|3297594.5| 0.0|   null|     10.22|      -0.06|     8905.0|  9101002.0|     -1.0|      null|       null|       null|       null|     null|\n",
      "|2018-12-10 09:25:05|       2.0|25.12| -0.22|2522.0|6335264.0|-1.0|  25.12|      null|       null|       null|       null|     null|      null|       null|       null|       null|     null|\n",
      "|2018-12-10 09:30:00|       2.0|25.12|   0.0|  49.0| 123129.0|-1.0|  25.16|     25.12|      -0.22|     2522.0|  6335264.0|     -1.0|      null|       null|       null|       null|     null|\n",
      "|2018-12-10 09:30:05|       2.0|25.16|  0.01| 379.5| 954694.0| 0.0|   null|     25.12|        0.0|       49.0|   123129.0|     -1.0|     25.12|      -0.22|     2522.0|  6335264.0|     -1.0|\n",
      "|2018-12-10 09:30:05|       4.0|16.17| -0.06|   7.5|  12145.5|-1.0|   null|      null|       null|       null|       null|     null|      null|       null|       null|       null|     null|\n",
      "|2018-12-10 09:25:05|       5.0|  3.0| -0.02|  47.0|  14100.0|-1.0|   2.99|      null|       null|       null|       null|     null|      null|       null|       null|       null|     null|\n",
      "|2018-12-10 09:30:05|       5.0| 2.99| -0.01| 113.0|  33698.5|-1.0|   null|       3.0|      -0.02|       47.0|    14100.0|     -1.0|      null|       null|       null|       null|     null|\n",
      "|2018-12-10 09:25:05|       6.0| 5.66| -0.05| 639.0| 361674.0|-1.0|   5.64|      null|       null|       null|       null|     null|      null|       null|       null|       null|     null|\n",
      "|2018-12-10 09:30:05|       6.0| 5.64| -0.01| 982.5| 555991.5|-1.0|   null|      5.66|      -0.05|      639.0|   361674.0|     -1.0|      null|       null|       null|       null|     null|\n",
      "|2018-12-10 09:25:05|       7.0| 7.83| -0.17|1035.0| 810405.0| 1.0|   7.83|      null|       null|       null|       null|     null|      null|       null|       null|       null|     null|\n",
      "|2018-12-10 09:30:00|       7.0| 7.83|   0.0|  23.0|  18009.0|-1.0|   7.82|      7.83|      -0.17|     1035.0|   810405.0|      1.0|      null|       null|       null|       null|     null|\n",
      "|2018-12-10 09:30:05|       7.0| 7.82| -0.01|1025.0| 800953.0|-0.5|   null|      7.83|        0.0|       23.0|    18009.0|     -1.0|      7.83|      -0.17|     1035.0|   810405.0|      1.0|\n",
      "|2018-12-10 09:25:05|       8.0| 3.89| -0.03| 569.0| 221341.0|-1.0|   3.88|      null|       null|       null|       null|     null|      null|       null|       null|       null|     null|\n",
      "|2018-12-10 09:30:05|       8.0| 3.88|   0.0| 524.5| 203609.5| 0.0|   null|      3.89|      -0.03|      569.0|   221341.0|     -1.0|      null|       null|       null|       null|     null|\n",
      "|2018-12-10 09:25:05|       9.0| 4.78|   0.0| 256.0| 122368.0|-1.0|   4.75|      null|       null|       null|       null|     null|      null|       null|       null|       null|     null|\n",
      "|2018-12-10 09:30:05|       9.0| 4.75| -0.02|  68.0|  32293.5| 0.0|   null|      4.78|        0.0|      256.0|   122368.0|     -1.0|      null|       null|       null|       null|     null|\n",
      "|2018-12-10 09:25:05|      10.0| 3.84| -0.05|1153.0| 442752.0|-1.0|   3.85|      null|       null|       null|       null|     null|      null|       null|       null|       null|     null|\n",
      "|2018-12-10 09:30:00|      10.0| 3.85|  0.01|  18.0|   6930.0| 1.0|   3.85|      3.84|      -0.05|     1153.0|   442752.0|     -1.0|      null|       null|       null|       null|     null|\n",
      "|2018-12-10 09:30:05|      10.0| 3.85|   0.0| 321.0| 123428.0| 1.0|   null|      3.85|       0.01|       18.0|     6930.0|      1.0|      3.84|      -0.05|     1153.0|   442752.0|     -1.0|\n",
      "+-------------------+----------+-----+------+------+---------+----+-------+----------+-----------+-----------+-----------+---------+----------+-----------+-----------+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"stock_code\").orderBy('time')#.rangeBetween(-3, 1)\n",
    "lead1_Y = lead(col('price'), count=1).over(w)\n",
    "df_fe = df.withColumn('lead1-Y', lead1_Y)\n",
    "\n",
    "def append_lags(df_fe, i):\n",
    "    except_set = set(['time', 'stock_code'])\n",
    "    for c in df.columns:\n",
    "        if c in except_set:\n",
    "            continue\n",
    "        df_fe = df_fe.withColumn(f\"{c}-lag{i}\", lag(col(c), count=i).over(w))\n",
    "    return df_fe\n",
    "\n",
    "for i in range(2):\n",
    "    df_fe = append_lags(df_fe, i+1)\n",
    "\n",
    "df_fe = df_fe.orderBy(col('stock_code'), col('time'), ascending=[1, 1])\n",
    "df_fe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-01T12:13:20.649066Z",
     "start_time": "2019-05-01T12:13:15.731054Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[datetime.datetime(2018, 12, 10, 9, 25, 5),\n",
       "  1.0,\n",
       "  10.22,\n",
       "  -0.06,\n",
       "  8905.0,\n",
       "  9101002.0,\n",
       "  -1.0,\n",
       "  10.21,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None],\n",
       " [datetime.datetime(2018, 12, 10, 9, 30, 5),\n",
       "  1.0,\n",
       "  10.21,\n",
       "  0.0,\n",
       "  3229.0,\n",
       "  3297594.5,\n",
       "  0.0,\n",
       "  None,\n",
       "  10.22,\n",
       "  -0.06,\n",
       "  8905.0,\n",
       "  9101002.0,\n",
       "  -1.0,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None],\n",
       " [datetime.datetime(2018, 12, 10, 9, 25, 5),\n",
       "  2.0,\n",
       "  25.12,\n",
       "  -0.22,\n",
       "  2522.0,\n",
       "  6335264.0,\n",
       "  -1.0,\n",
       "  25.12,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None],\n",
       " [datetime.datetime(2018, 12, 10, 9, 30),\n",
       "  2.0,\n",
       "  25.12,\n",
       "  0.0,\n",
       "  49.0,\n",
       "  123129.0,\n",
       "  -1.0,\n",
       "  25.16,\n",
       "  25.12,\n",
       "  -0.22,\n",
       "  2522.0,\n",
       "  6335264.0,\n",
       "  -1.0,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None],\n",
       " [datetime.datetime(2018, 12, 10, 9, 30, 5),\n",
       "  2.0,\n",
       "  25.16,\n",
       "  0.01,\n",
       "  379.5,\n",
       "  954694.0,\n",
       "  0.0,\n",
       "  None,\n",
       "  25.12,\n",
       "  0.0,\n",
       "  49.0,\n",
       "  123129.0,\n",
       "  -1.0,\n",
       "  25.12,\n",
       "  -0.22,\n",
       "  2522.0,\n",
       "  6335264.0,\n",
       "  -1.0]]"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ks = df.columns\n",
    "def map_df_to_rdd(v):\n",
    "    d = v.asDict()\n",
    "    r = []\n",
    "    for k in d:\n",
    "        r.append(v[k])\n",
    "    return r\n",
    "\n",
    "# transform back to rdd\n",
    "df_fe.rdd.map(map_df_to_rdd).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-01T06:15:36.778457Z",
     "start_time": "2019-05-01T06:15:36.775985Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# r = df_fe.rdd.map(lambda v: v).take(5)\n",
    "# r = r[0]\n",
    "# r = r.asDict()\n",
    "# # for v in r:\n",
    "# #     print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### feature engineer TRANSFORMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T12:42:14.753533Z",
     "start_time": "2019-05-04T12:42:14.191541Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load data as rdd\n",
    "# rdd = sc.textFile(pjoin(data_dir, 'kmeans_data_test.txt'))\n",
    "# rddQueue = rdd.randomSplit(np.ones(200), seed=5003)\n",
    "batchInterval = 3\n",
    "rddQueue = list(rddg(rdd, keys, batchInterval))\n",
    "# make rdd as stream\n",
    "ssc = StreamingContext(sc, batchInterval)\n",
    "dataStream = ssc.queueStream(rddQueue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T12:42:14.792809Z",
     "start_time": "2019-05-04T12:42:14.756304Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fe\n",
    "fields = ['time','stock_code','price','change','volume','amount','type']\n",
    "field_types = [TimestampType(), DoubleType(), DoubleType(), DoubleType(), DoubleType(), DoubleType(), DoubleType()]\n",
    "sfields = []\n",
    "for f, t in zip(fields, field_types):\n",
    "    sfields.append(StructField(f, t, True))\n",
    "schema = StructType(sfields)\n",
    "\n",
    "def append_lags(df_fe, fields, w, i):\n",
    "    except_set = set(['time', 'stock_code'])\n",
    "    for c in fields:#df.columns:\n",
    "        if c in except_set:\n",
    "            continue\n",
    "        df_fe = df_fe.withColumn(f\"{c}-lag{i}\", lag(col(c), count=i).over(w))\n",
    "    return df_fe\n",
    "\n",
    "def map_df_to_rdd(v):\n",
    "    d = v.asDict()\n",
    "    r = []\n",
    "    for k in d:#fields:\n",
    "        print(k)\n",
    "        r.append(v[k])\n",
    "    return r\n",
    "\n",
    "def feature_builder(rdd, schema):\n",
    "    # 1. aggregation\n",
    "    # print((rdd.take(5)))\n",
    "    df = rdd.toDF(schema=schema)    \n",
    "    df = df.groupBy(col('time'), col('stock_code'))\\\n",
    "    .avg('price', 'change', 'volume', 'amount', 'type')\\\n",
    "    .select(col('time'), col('stock_code'),\n",
    "            bround('avg(price)', 2).alias('price'), \n",
    "            bround('avg(change)', 2).alias('change'),\n",
    "            bround('avg(volume)', 2).alias('volume'),\n",
    "            bround('avg(amount)', 2).alias('amount'),\n",
    "            bround('avg(type)', 2).alias('type'))\\\n",
    "    .orderBy(col('stock_code'), col('time'), ascending=[1, 1])\n",
    "    \n",
    "    # 2. shift features\n",
    "    Y_COL_NAME = 'lead1-Y'\n",
    "    w = Window.partitionBy(\"stock_code\").orderBy('time')#.rangeBetween(3, 1)\n",
    "    lead1_Y = lead(col('price'), count=1).over(w)\n",
    "    df_fe = df.withColumn(Y_COL_NAME, lead1_Y)\n",
    "    for i in range(batchInterval-1): #2): # shiftting time span, better to be the batchInterval-1 \n",
    "        df_fe = append_lags(df_fe, schema.names, w, i+1)\n",
    "    df_fe = df_fe.orderBy(col('stock_code'), col('time'), ascending=[1, 1])\n",
    "    \n",
    "    # 3. move lable column Y (lead1-Y) to the last column\n",
    "    cols = df_fe.columns\n",
    "    cols.remove(Y_COL_NAME)\n",
    "    cols.append(Y_COL_NAME)\n",
    "    df_fe = df_fe.select(cols)\n",
    "    # df_fe.show()\n",
    "\n",
    "    # 4. transback to rdd\n",
    "    rdd_out = df_fe.rdd.map(map_df_to_rdd)\n",
    "        \n",
    "    # 5. drop rows with None feature values after shift. Except for lead-Y, which is the last col of the rdd\n",
    "    rdd_out = rdd_out.filter(lambda v: None not in v[:-1])\n",
    "    \n",
    "    # 6. drop time column, which is the first column.\n",
    "#     rdd_out = rdd_out.map(lambda v: v[1:])\n",
    "    \n",
    "    return rdd_out\n",
    "\n",
    "# using slide window to ensure the new coming data have enough predecessor to shift.\n",
    "# window width is twice as the batchInterval where each interval slide only 1 time of the batchInterval\n",
    "dataStream = dataStream.window(batchInterval*2, batchInterval)\n",
    "dataStream = dataStream.transform(lambda rdd: feature_builder(rdd, schema))\n",
    "dataStream = dataStream.cache()\n",
    "# after transform shiftting features, drop the old useless data\n",
    "# dataStream = dataStream.window(batchInterval)\n",
    "# dataStream.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### split train/pred data stream for KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T12:42:14.799672Z",
     "start_time": "2019-05-04T12:42:14.795045Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we make an input stream of vectors for training,\n",
    "# as well as a stream of vectors for testing\n",
    "def cluster_parse_pred(lp):\n",
    "    feature_start_idx = 2\n",
    "    # stock id; the 0th is time\n",
    "    label = lp[1] # float(lp[lp.find('(') + 1: lp.find(')')])\n",
    "    # features\n",
    "    vec = Vectors.dense(lp[feature_start_idx:]) # Vectors.dense(lp[lp.find('[') + 1: lp.find(']')].split(','))\n",
    "    return LabeledPoint(label, vec)\n",
    "\n",
    "def cluster_parse_train(lp):\n",
    "    feature_start_idx = 2\n",
    "    # only features for clustering. here include the training Y but filterd None before call this function\n",
    "    vec = Vectors.dense(lp[feature_start_idx:]) # Vectors.dense([float(x) for x in lp[lp.find('[') + 1: lp.find(']')].split(',')])\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T12:42:14.805584Z",
     "start_time": "2019-05-04T12:42:14.802088Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# separate to 2 streams\n",
    "nonone_datastream = dataStream.filter(lambda v: None not in v)\n",
    "trainS = nonone_datastream.map(cluster_parse_train)\n",
    "predS = nonone_datastream.map(cluster_parse_pred).map(lambda lp: (lp.label, lp.features))\n",
    "\n",
    "# trainS.pprint()\n",
    "# predS.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### clustering\n",
    "\n",
    "1. **dim** may need to change due the different feature Engineering**\n",
    "2. since 1 label could have **multiple** lines of data (different time), may result in same label belong to different clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T12:42:15.360763Z",
     "start_time": "2019-05-04T12:42:15.357433Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ssc.checkpoint('cp')\n",
    "# initial streaming clustering model\n",
    "# here dim may change due the different feature Engineering; \n",
    "# for now is features except (time, stock_code) then shift (batchInterval-1) times combine with current value with another one future price.\n",
    "dim = (len(schema.names) - 2)*batchInterval + 1\n",
    "numClusters = 2\n",
    "model = StreamingKMeans(k=numClusters, decayFactor=1.0).setRandomCenters(dim, 1.0, 5003) # dim weight seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T12:42:15.618608Z",
     "start_time": "2019-05-04T12:42:15.606972Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train & pred on streams get result: dstream ==> (label, clusterID)\n",
    "model.trainOn(trainS)\n",
    "result = model.predictOnValues(predS)\n",
    "# the result will be like same label have multiple clusters, which each row belongs to 1 timepoint.\n",
    "# therefore, requring a groupby to eliminate the duplicated labels (stock_code). \n",
    "# using mode as cluster for the stock; require scipy.stats\n",
    "result = result.groupByKey().mapValues(lambda v: float(stats.mode(list(v)).mode[0]))\n",
    "# result.pprint()\n",
    "# result.foreachRDD(lambda rd: print(rd.lookup(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### aggregation by cls & time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T12:42:16.533187Z",
     "start_time": "2019-05-04T12:42:16.502258Z"
    }
   },
   "outputs": [],
   "source": [
    "# swap time & stock_code, then make stock_code as key\n",
    "kstream = dataStream.map(lambda v: [v[1], v[0], v[1:]]).map(flattenLine).map(lambda v: (v[0], v[1:]))\n",
    "# kstream.pprint()\n",
    "# join the datapoin with label\n",
    "# (stock_code, (cluster_id, [time, stock_code, features:list, label]))\n",
    "jstream = result.join(kstream)\n",
    "# [cluster_id, time, stock_code, features:list, label], remove extra stock_code\n",
    "jstream = jstream.map(flattenLine).map(lambda v: v[1:])\n",
    "# jstream.pprint()\n",
    "\n",
    "rfields = ['cluster_id', 'time', 'stock_code', \n",
    "          'price', 'change', 'volume', 'amount', 'type', \n",
    "          'price-lag1', 'change-lag1', 'volume-lag1', 'amount-lag1', 'type-lag1', \n",
    "          'price-lag2', 'change-lag2', 'volume-lag2', 'amount-lag2', 'type-lag2',\n",
    "          'lead1-Y']\n",
    "rfield_types = [DoubleType(), TimestampType(), DoubleType(), \n",
    "               DoubleType(), DoubleType(), DoubleType(), DoubleType(), DoubleType(),\n",
    "               DoubleType(), DoubleType(), DoubleType(), DoubleType(), DoubleType(), \n",
    "               DoubleType(), DoubleType(), DoubleType(), DoubleType(), DoubleType(), \n",
    "               DoubleType()]\n",
    "rFields = []\n",
    "for f, t in zip(rfields, rfield_types):\n",
    "    rFields.append(StructField(f, t, True))\n",
    "schema_agg = StructType(rFields)\n",
    "\n",
    "# aggregation & feature engineering for Regression model.\n",
    "def agg_fe_by_clusters_time(rdd, schema):\n",
    "    try:\n",
    "        df = rdd.toDF(schema=schema)\n",
    "        df = df.groupBy(col('cluster_id'), col('time'))\\\n",
    "        .avg('price', 'change', 'volume', 'amount', 'type', \n",
    "             'price-lag1', 'change-lag1', 'volume-lag1', 'amount-lag1', 'type-lag1',\n",
    "             'price-lag2', 'change-lag2', 'volume-lag2', 'amount-lag2', 'type-lag2',\n",
    "             'lead1-Y')\\\n",
    "        .select(col('cluster_id'), col('time'),\n",
    "                bround('avg(price)', 2).alias('price'), \n",
    "                bround('avg(change)', 2).alias('change'),\n",
    "                bround('avg(volume)', 2).alias('volume'),\n",
    "                bround('avg(amount)', 2).alias('amount'),\n",
    "                bround('avg(type)', 2).alias('type'),\n",
    "                bround('avg(price-lag1)', 2).alias('price-lag1'), \n",
    "                bround('avg(change-lag1)', 2).alias('change-lag1'),\n",
    "                bround('avg(volume-lag1)', 2).alias('volume-lag1'),\n",
    "                bround('avg(amount-lag1)', 2).alias('amount-lag1'),\n",
    "                bround('avg(type-lag1)', 2).alias('type-lag1'),\n",
    "                bround('avg(price-lag2)', 2).alias('price-lag2'), \n",
    "                bround('avg(change-lag2)', 2).alias('change-lag2'),\n",
    "                bround('avg(volume-lag2)', 2).alias('volume-lag2'),\n",
    "                bround('avg(amount-lag2)', 2).alias('amount-lag2'),\n",
    "                bround('avg(type-lag2)', 2).alias('type-lag2'),\n",
    "                bround('avg(lead1-Y)', 2).alias('lead1-Y'))\\\n",
    "        .orderBy(col('cluster_id'), col('time'), ascending=[1, 1])\n",
    "        # df.show()\n",
    "        rdd_out = df.rdd.map(map_df_to_rdd)\n",
    "        return rdd_out\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return rdd\n",
    "\n",
    "rdatastream = jstream.transform(lambda rdd: agg_fe_by_clusters_time(rdd, schema_agg))\n",
    "rdatastream = rdatastream.cache()\n",
    "# rdatastream.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### split data for clusters. stream branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T12:42:17.253562Z",
     "start_time": "2019-05-04T12:42:17.235513Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# (y, vec) parser\n",
    "def rparse(lp):\n",
    "    y = -1 if lp[-1] is None else lp[-1]\n",
    "    vec = Vectors.dense(lp[:-1])\n",
    "    return LabeledPoint(y, vec)\n",
    "\n",
    "# must use a function to divide the branches of stream \n",
    "# to avoid using the the global varibale `cls` accidently\n",
    "def branch(gstream, cls):\n",
    "    # 0: cluster_id; 1: time; 2:-1 -> features; -1: Y(label)\n",
    "    return gstream.filter(lambda v: v[0] == cls).map(lambda v: rparse(v[2:]))\n",
    "    \n",
    "    \n",
    "# separate steamdata for different cluster\n",
    "cls_streams = dict()\n",
    "for cls in range(numClusters):\n",
    "    clsstream = branch(rdatastream, cls)\n",
    "    cls_streams[cls] = clsstream\n",
    "    clsstream.cache()\n",
    "#     clsstream.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T12:42:18.247858Z",
     "start_time": "2019-05-04T12:42:18.190545Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# split train/pred data stream for REGRESSION\n",
    "def rparse_pred(stream):\n",
    "    # none label row is the row to predict\n",
    "    return stream.filter(lambda v: v.label == -1).map(lambda v: (v.label, v.features))\n",
    "\n",
    "def rparse_train(stream):\n",
    "    # rows with labels are the rows to train\n",
    "    return stream.filter(lambda v: v.label != -1)#.map(lambda v: (v.label, v.features))\n",
    "\n",
    "# features exclude colume 'cluster_id', 'time', 'stock_code', 'lead1-Y'\n",
    "numFeatures = len(schema_agg.names) - 4\n",
    "def make_reg_model(stream, numFeatures):\n",
    "    # inital streaming linear regression model\n",
    "    mdl = StreamingLinearRegressionWithSGD();\n",
    "    mdl.setInitialWeights(np.zeros(numFeatures).tolist())\n",
    "    mdl.trainOn(stream)\n",
    "    return mdl\n",
    "\n",
    "cls_train_stream = dict()\n",
    "cls_pred_stream = dict()\n",
    "cls_reg_models = dict()\n",
    "cls_reg_prediction = dict()\n",
    "for cls in range(numClusters):\n",
    "    cls_train_stream[cls] = rparse_train(cls_streams[cls])\n",
    "    cls_pred_stream[cls] = rparse_pred(cls_streams[cls])\n",
    "    \n",
    "    cls_train_stream[cls].pprint()\n",
    "    cls_pred_stream[cls].pprint()\n",
    "    \n",
    "    # reg model\n",
    "    cls_reg_models[cls] = make_reg_model(cls_train_stream[cls], numFeatures)\n",
    "    # cls_reg_models[cls].predictOn(cls_pred_stream[cls].map(lambda lp: lp.features))\n",
    "    cls_reg_prediction[cls] = cls_reg_models[cls].predictOnValues(cls_pred_stream[cls])\n",
    "    # valstream\n",
    "    cls_reg_prediction[cls].pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### encapsulate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encapsulate_result(self, cls, real_stream, pred_val_stream):\n",
    "    real_stream.foreachRDD(lambda rdd: self.encapsulate_real(cls, rdd))\n",
    "    # find time to predict -> (label=-1, time)\n",
    "    pred_time_stream = real_stream.filter(lambda v: v[1].label == -1).map(lambda v: (v[1].label, dt.strftime(v[0], '%Y-%m-%d %H:%M:%S')))\n",
    "    pred_stream = pred_time_stream.join(pred_val_stream)\n",
    "    pred_stream.pprint()\n",
    "    # pred_stream.foreachRDD(lambda rdd: self.encapsulate_pred(cls, rdd))\n",
    "\n",
    "def encapsulate_real(self, cls, rdd):\n",
    "    res_dic = self.run_result#.result_dict\n",
    "    cls_dic = res_dic.get(cls, dict())\n",
    "    # (time_dt, (label=real_price, features)) -> (time, real_price)\n",
    "    data = rdd.map(lambda v: (dt.strftime(v[0], '%Y-%m-%d %H:%M:%S'), v[1].label)).collect()\n",
    "    for time, val in data:\n",
    "        data_dic = cls_dic.get(time: dict())\n",
    "        data_dic.update({'real':val})\n",
    "        cls_dic.update({time, data_dic})\n",
    "    res_dic.update({cls: cls_dic})\n",
    "    print(f\"{res_dic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T12:43:33.595391Z",
     "start_time": "2019-05-04T12:42:19.117943Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-05-04 20:42:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-04 20:42:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-04 20:42:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-04 20:42:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-04 20:42:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-04 20:42:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-04 20:42:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-04 20:42:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-04 20:42:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-04 20:42:24\n",
      "-------------------------------------------\n",
      "(10.03,[10.03,0.0,1213.65,950145.73,-0.17,10.04,0.0,146.09,123027.18,-0.22,10.04,-0.09,2139.81,1627101.94,-0.48])\n",
      "(9.29,[9.25,0.0,128.14,95144.92,0.07,9.25,-0.01,755.06,542346.82,-0.22,9.26,-0.05,575.28,365594.16,-0.55])\n",
      "(9.24,[9.12,0.0,95.14,68661.92,-0.13,9.12,0.0,150.57,109746.48,0.05,9.12,-0.01,757.1,551982.31,-0.29])\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-04 20:42:24\n",
      "-------------------------------------------\n",
      "(-1.0, DenseVector([9.09, 0.0, 93.71, 63854.48, -0.1, 9.1, 0.0, 95.38, 66816.68, -0.11, 9.1, 0.0, 192.83, 128536.98, -0.01]))\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-04 20:42:24\n",
      "-------------------------------------------\n",
      "(-1.0, 0.0)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-04 20:42:27\n",
      "-------------------------------------------\n",
      "(9.08,[9.08,0.0,72.15,48461.27,-0.06,9.09,0.0,71.42,45908.47,-0.11,9.09,0.0,99.23,67781.49,0.07])\n",
      "(9.3,[9.23,0.0,80.24,55009.16,-0.01,9.23,0.0,73.1,48513.56,-0.08,9.23,0.0,70.96,44688.81,-0.09])\n",
      "(9.22,[9.14,0.0,90.91,56273.62,0.02,9.13,0.0,74.15,52421.03,-0.05,9.13,0.0,73.86,49417.36,-0.08])\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-04 20:42:27\n",
      "-------------------------------------------\n",
      "(-1.0, DenseVector([8.91, 0.0, 84.49, 55924.22, 0.06, 8.92, 0.0, 90.32, 54576.62, 0.02, 8.91, 0.0, 68.83, 47970.37, -0.09]))\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-04 20:42:27\n",
      "-------------------------------------------\n",
      "(-1.0, 0.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:    \n",
    "    ssc.start()\n",
    "    # ssc.stop(=True, stopGraceFully=True)\n",
    "    ssc.awaitTermination(50)\n",
    "    ssc.stop(stopSparkContext=False)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    ssc.stop(stopSparkContext=False)\n",
    "\n",
    "\n",
    "# mdl = model.latestModel()\n",
    "# print(f\"centers: {mdl.clusterCenters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### other tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-01T12:28:06.650007Z",
     "start_time": "2019-05-01T12:27:43.849Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mdl.centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-01T12:28:06.651288Z",
     "start_time": "2019-05-01T12:27:43.898Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import Normalizer\n",
    "v = Vectors.dense(range(3))\n",
    "nor = Normalizer(1)\n",
    "nor.transform(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-01T12:28:06.652297Z",
     "start_time": "2019-05-01T12:27:43.965Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([v])\n",
    "nor.transform(rdd).collect()\n",
    "nor2 = Normalizer(float(\"inf\"))\n",
    "nor2.transform(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T03:17:02.119142Z",
     "start_time": "2019-04-30T03:17:02.115080Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0, 1.0, 2.0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T03:18:39.595570Z",
     "start_time": "2019-04-30T03:18:39.589765Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2.],\n",
       "       [0., 1., 2.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(map(lambda v: v.array, [v, v])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T03:18:56.651118Z",
     "start_time": "2019-04-30T03:18:56.646372Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array(list(map(lambda v: v.array, [v]))), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T09:28:45.576492Z",
     "start_time": "2019-05-04T09:28:45.570209Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "4\n",
      "3\n",
      "v5:55\n",
      "v6:66\n",
      "v7:929\n",
      "v8:100\n",
      "**********\n",
      "929\n",
      "v5:55\n",
      "v6:66\n",
      "v8:100\n"
     ]
    }
   ],
   "source": [
    "# def call(*vals):\n",
    "#     for v in vals:\n",
    "#         print(v)\n",
    "# call(*schema.fields)\n",
    "# np.ones(2).astype('int').tolist()\n",
    "\n",
    "def call2(v7, **kwargs):\n",
    "    print('*'*10)\n",
    "    print(v7)\n",
    "    for p in kwargs:\n",
    "        print(f\"{p}:{kwargs[p]}\")\n",
    "\n",
    "def call(v1, v2=2, v3=3, **kwargs):\n",
    "    print(v1)\n",
    "    print(v2)\n",
    "    print(v3)\n",
    "    for p in kwargs:\n",
    "        print(f\"{p}:{kwargs[p]}\")\n",
    "    \n",
    "    call2(**kwargs)\n",
    "        \n",
    "pp = {'v7': 929, 'v8': 100}\n",
    "call(12, v5=55, v3=3, v2=4, v6=66, **pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
